{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifleSoup4 를 이용한 웹페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 모듈을 로딩한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 텍스트 라인은 샘플 HTML 문서이다. 실제로는 저장된 HTML파일을 불러오거나 웹에서 바로 HTML문서를 다운로드 받아 데이터 처리를 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_doc = \"<html><body><h1>Mr. Belvedere Fan Club</h1><div id='nav'>navigation bar</div><div class='nav'>navigation class</div><div class='header'><a href='twitter_anywhere'>my twitter</a></div></body></html>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup의 \"html.parser\"를 이용하여 문서를 parsing한다. \"html.parser\" 외에 \"xml\", \"html5lib\" 등의 parser를 제공한다.\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsing된 HTML 문서를 보기 좋게 보여준다.\n",
    "\n",
    "range를 사용하여 원하는 만큼만 볼 수 있다. soup.prettify()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문서에서 ```<h1>``` 인 요소들만 찾아 list로 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading = soup.find_all(\"h1\")\n",
    "heading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요소의 내용(text)를 반환하기 위해서 get_text()를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<질문> ```heading.get_text()``` 는 에러가 나는 이유는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 요소 중, class와 id 등으로 filtering 하기 위해서 두번째 패러미터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", id=\"nav\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs = soup.find_all(\"div\", class_=\"header\")\n",
    "divs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_list = []\n",
    "for div in divs:\n",
    "    if div.a[\"href\"] == \"twitter_anywhere\":\n",
    "        id_list.append(div.a.text) # text 는 get_text()와 동일하게 사용됨.\n",
    "id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL 가져와서 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import ssl\n",
    "\n",
    "context = ssl._create_unverified_context()\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\", context=context) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    strongs = soup.find_all(\"strong\", class_=\"tit_thumb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for strong in strongs:\n",
    "    print(strong)\n",
    "#    print(strong.a.text)\n",
    "#    print(strong.a[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 1: 데이터 수집을 위한 리스트 작성\n",
    "\n",
    "위의 주소에서 수집하고자 하는 URL의 리스트를 작성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_list = []\n",
    "with urllib.request.urlopen(\"https://media.daum.net/ranking/bestreply/\", context=context) as url:\n",
    "    # 아래에 코드 작성\n",
    "    \n",
    "        \n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 2: 기사 수집 1\n",
    "\n",
    "#### 주어진 URL의 기사의 타이틀을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url, context=context) as url:\n",
    "    #아래에 코드 작성\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 언론사 이름을 수집해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://v.media.daum.net/v/20181103172202580\"\n",
    "\n",
    "with urllib.request.urlopen(base_url, context=context) as url:\n",
    "    doc = url.read()\n",
    "    soup = BeautifulSoup(doc, \"html.parser\")\n",
    "    cp = soup.find_all(\"em\", class_=\"info_cp\")[0]\n",
    "    print(cp.a.img['alt'])\n",
    "    print(cp.img[\"alt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
